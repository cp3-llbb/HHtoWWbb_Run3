{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22512f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 09:12:46.757468: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.3-18d03/x86_64-centos7-gcc11-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.3-94809/x86_64-centos7-gcc11-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/jaxlib/mlir/_mlir_libs:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/onnxruntime/capi/:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u312-80070/x86_64-centos7-gcc11-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.37-355ed/x86_64-centos7/lib:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/lib64:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/lib:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib64:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib:/cvmfs/sft.cern.ch/lcg/releases/R/4.1.2-234e4/x86_64-centos7-gcc11-opt/lib64/R/library/readr/rcon\n",
      "2023-09-12 09:12:46.757533: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import optparse\n",
    "import yaml\n",
    "import importlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5398a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage = 'usage: %prog [options]'\n",
    "# parser = optparse.OptionParser(usage)\n",
    "\n",
    "# parser.add_option('-o', '--outDir',\n",
    "#                     dest='outputPath',\n",
    "#                     help='output directory',\n",
    "#                     type='string')\n",
    "\n",
    "# (opt, args) = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d7a4d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 input variables\n"
     ]
    }
   ],
   "source": [
    "### Parameters of the training ###\n",
    "\n",
    "#split = \"even\" \n",
    "split = \"odd\" \n",
    "# split = even | odd -> on what split to train the model (will be in the name)\n",
    "# -> you need one \"odd\" and one \"even\" models to be put inside bamboo\n",
    "\n",
    "suffix = 'test1'\n",
    "# Suffix that will be added to the saved model (so multiple DNNs can be trained)\n",
    "\n",
    "quantile = 1.0 # We will repeat the part of the weights rightmost tail\n",
    "# Eg : 0.95, means we take the 5% events on the right tail of training weight and repeat them\n",
    "# 1.0 means no correction (to be used if you want to disable it)\n",
    "\n",
    "tags = ['HH','background']\n",
    "\n",
    "# DNN hyperparameters #\n",
    "parameters = {\n",
    "    'epochs'                : 200,\n",
    "    'lr'                    : 0.001,\n",
    "    'batch_size'            : 256,\n",
    "    'n_layers'              : 3,\n",
    "    'n_neurons'             : 64,\n",
    "    'hidden_activation'     : 'relu',\n",
    "    'output_activation'     : 'softmax',\n",
    "    'l2'                    : 1e-6,\n",
    "    'dropout'               : 0.,\n",
    "    'batch_norm'            : True,\n",
    "}\n",
    "# Input variables\n",
    "input_vars=[\n",
    "    \"ak4bjet1_pt\",\n",
    "    \"ak4bjet1_eta\",\n",
    "    \"ak4bjet1_phi\",\n",
    "    \"leadingLepton_pt\",\n",
    "    \"leadingLepton_eta\",\n",
    "    \"leadingLepton_phi\",\n",
    "    \"subleadingLepton_pt\",\n",
    "    \"subleadingLepton_eta\",\n",
    "    \"subleadingLepton_phi\"\n",
    "    ]\n",
    "\n",
    "print(f'Using {len(input_vars)} input variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac770a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using skim file ../output/mvaSkim-invMleptons/results/DL_resolved.parquet\n",
      "Using yaml file ../output/mvaSkim-invMleptons/plots.yml\n"
     ]
    }
   ],
   "source": [
    "outputPath = '../output/mvaSkim-invMleptons'\n",
    "yamlFile = os.path.join(outputPath,'plots.yml')\n",
    "skimFile = os.path.join(outputPath,'results/DL_resolved.parquet')\n",
    "# yamlFile = os.path.join(opt.outputPath,'plots.yml')\n",
    "# skimFile = os.path.join(opt.outputPath,'results/DL_resolved.parquet')\n",
    "\n",
    "print(f'Using skim file {skimFile}')\n",
    "print(f'Using yaml file {yamlFile}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "100132f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe from parquet file\n",
    "df = pd.read_parquet(skimFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0aab42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['background', 'HH'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add tag column #\n",
    "df['tag'] = 'background'\n",
    "df.loc[df.process.str.contains('HH'),['tag']] = 'HH'\n",
    "df[\"tag\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb21a819",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(set(tags).intersection(set(pd.unique(df['tag'])))) == len(tags) # Just cross check to avoid mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ad58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding is a way to convert a column to a format that is easier for machine learning applications\n",
    "# Here I transform the tag (bkg or signal) column to binary and add it as new columns\n",
    "\n",
    "one_hot = pd.get_dummies(df['tag'], dtype=float)\n",
    "df = pd.concat((df,one_hot),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d130c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events with negative weight is : 1997\n"
     ]
    }
   ],
   "source": [
    "Nevents_before_weight_cut = df.shape[0]\n",
    "df = df[df.weight > 0] # remove events with negative weight\n",
    "Nevents_after_weight_cut = df.shape[0]\n",
    "Nevents_with_negative_weight = Nevents_before_weight_cut - Nevents_after_weight_cut\n",
    "print(f\"Number of events with negative weight is : {Nevents_with_negative_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5e7ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the weight column as event weight to calculate it later\n",
    "df['event_weight'] = df['weight'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0fdd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (df['event_weight'] < 0).sum() > 0:\n",
    "    raise RuntimeError(f\"There are {(df['event_weight'] < 0).sum()} events with negative event weight, this should not happen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49ecea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the event weight column as training weight\n",
    "if 'training_weight' in df.columns:\n",
    "    del df['training_weight']\n",
    "    \n",
    "df['training_weight'] = df['event_weight'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6dc6b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ak4bjet1_eta</th>\n",
       "      <th>ak4bjet1_phi</th>\n",
       "      <th>ak4bjet1_pt</th>\n",
       "      <th>leadingLepton_eta</th>\n",
       "      <th>leadingLepton_phi</th>\n",
       "      <th>leadingLepton_pt</th>\n",
       "      <th>subleadingLepton_eta</th>\n",
       "      <th>subleadingLepton_phi</th>\n",
       "      <th>subleadingLepton_pt</th>\n",
       "      <th>weight</th>\n",
       "      <th>process</th>\n",
       "      <th>tag</th>\n",
       "      <th>HH</th>\n",
       "      <th>background</th>\n",
       "      <th>event_weight</th>\n",
       "      <th>training_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.425537</td>\n",
       "      <td>-1.323486</td>\n",
       "      <td>126.18750</td>\n",
       "      <td>-0.161804</td>\n",
       "      <td>0.887695</td>\n",
       "      <td>28.389597</td>\n",
       "      <td>-0.295227</td>\n",
       "      <td>-1.368896</td>\n",
       "      <td>19.328093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>data</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.830356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.196289</td>\n",
       "      <td>-0.498291</td>\n",
       "      <td>48.59375</td>\n",
       "      <td>0.316284</td>\n",
       "      <td>-2.869141</td>\n",
       "      <td>109.997726</td>\n",
       "      <td>-0.364014</td>\n",
       "      <td>-0.565308</td>\n",
       "      <td>38.170570</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>data</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.830356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.066406</td>\n",
       "      <td>-1.631836</td>\n",
       "      <td>59.71875</td>\n",
       "      <td>-0.137390</td>\n",
       "      <td>-2.448730</td>\n",
       "      <td>66.637535</td>\n",
       "      <td>0.118851</td>\n",
       "      <td>-1.097900</td>\n",
       "      <td>41.266010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>data</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.830356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.093506</td>\n",
       "      <td>1.346436</td>\n",
       "      <td>96.00000</td>\n",
       "      <td>0.023682</td>\n",
       "      <td>-0.669556</td>\n",
       "      <td>51.262684</td>\n",
       "      <td>0.521362</td>\n",
       "      <td>2.923340</td>\n",
       "      <td>50.410046</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>data</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.830356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.292480</td>\n",
       "      <td>1.234619</td>\n",
       "      <td>50.28125</td>\n",
       "      <td>0.571655</td>\n",
       "      <td>2.450684</td>\n",
       "      <td>202.467529</td>\n",
       "      <td>1.092041</td>\n",
       "      <td>-0.964966</td>\n",
       "      <td>44.808109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>data</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.830356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-1.385254</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>170.12500</td>\n",
       "      <td>1.255615</td>\n",
       "      <td>2.157227</td>\n",
       "      <td>84.034363</td>\n",
       "      <td>1.660156</td>\n",
       "      <td>-2.505859</td>\n",
       "      <td>34.569920</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>TW</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>0.793566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-1.146240</td>\n",
       "      <td>-2.797852</td>\n",
       "      <td>53.46875</td>\n",
       "      <td>0.484863</td>\n",
       "      <td>-1.209717</td>\n",
       "      <td>46.397449</td>\n",
       "      <td>0.027924</td>\n",
       "      <td>-2.815430</td>\n",
       "      <td>32.236477</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>TW</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>0.793566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.057259</td>\n",
       "      <td>2.118164</td>\n",
       "      <td>81.06250</td>\n",
       "      <td>1.116699</td>\n",
       "      <td>-0.744019</td>\n",
       "      <td>29.344406</td>\n",
       "      <td>2.087891</td>\n",
       "      <td>-1.204590</td>\n",
       "      <td>23.747927</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>TW</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>0.793566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>-0.156525</td>\n",
       "      <td>2.066406</td>\n",
       "      <td>73.37500</td>\n",
       "      <td>-1.173584</td>\n",
       "      <td>-0.252808</td>\n",
       "      <td>251.302795</td>\n",
       "      <td>-2.291504</td>\n",
       "      <td>-2.399902</td>\n",
       "      <td>25.274199</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>TW</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>0.793566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-0.512573</td>\n",
       "      <td>0.627197</td>\n",
       "      <td>101.18750</td>\n",
       "      <td>-2.462402</td>\n",
       "      <td>-2.556641</td>\n",
       "      <td>86.100891</td>\n",
       "      <td>1.199951</td>\n",
       "      <td>2.861816</td>\n",
       "      <td>34.208595</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>TW</td>\n",
       "      <td>background</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.023457</td>\n",
       "      <td>0.793566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>570473 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ak4bjet1_eta  ak4bjet1_phi  ak4bjet1_pt  leadingLepton_eta  \\\n",
       "0      -1.425537     -1.323486    126.18750          -0.161804   \n",
       "1       1.196289     -0.498291     48.59375           0.316284   \n",
       "2       1.066406     -1.631836     59.71875          -0.137390   \n",
       "3      -1.093506      1.346436     96.00000           0.023682   \n",
       "4       1.292480      1.234619     50.28125           0.571655   \n",
       "..           ...           ...          ...                ...   \n",
       "52     -1.385254     -1.250000    170.12500           1.255615   \n",
       "53     -1.146240     -2.797852     53.46875           0.484863   \n",
       "54      0.057259      2.118164     81.06250           1.116699   \n",
       "55     -0.156525      2.066406     73.37500          -1.173584   \n",
       "56     -0.512573      0.627197    101.18750          -2.462402   \n",
       "\n",
       "    leadingLepton_phi  leadingLepton_pt  subleadingLepton_eta  \\\n",
       "0            0.887695         28.389597             -0.295227   \n",
       "1           -2.869141        109.997726             -0.364014   \n",
       "2           -2.448730         66.637535              0.118851   \n",
       "3           -0.669556         51.262684              0.521362   \n",
       "4            2.450684        202.467529              1.092041   \n",
       "..                ...               ...                   ...   \n",
       "52           2.157227         84.034363              1.660156   \n",
       "53          -1.209717         46.397449              0.027924   \n",
       "54          -0.744019         29.344406              2.087891   \n",
       "55          -0.252808        251.302795             -2.291504   \n",
       "56          -2.556641         86.100891              1.199951   \n",
       "\n",
       "    subleadingLepton_phi  subleadingLepton_pt    weight process         tag  \\\n",
       "0              -1.368896            19.328093  1.000000    data  background   \n",
       "1              -0.565308            38.170570  1.000000    data  background   \n",
       "2              -1.097900            41.266010  1.000000    data  background   \n",
       "3               2.923340            50.410046  1.000000    data  background   \n",
       "4              -0.964966            44.808109  1.000000    data  background   \n",
       "..                   ...                  ...       ...     ...         ...   \n",
       "52             -2.505859            34.569920  0.023457      TW  background   \n",
       "53             -2.815430            32.236477  0.023457      TW  background   \n",
       "54             -1.204590            23.747927  0.023457      TW  background   \n",
       "55             -2.399902            25.274199  0.023457      TW  background   \n",
       "56              2.861816            34.208595  0.023457      TW  background   \n",
       "\n",
       "     HH  background  event_weight  training_weight  \n",
       "0   0.0         1.0      1.000000        33.830356  \n",
       "1   0.0         1.0      1.000000        33.830356  \n",
       "2   0.0         1.0      1.000000        33.830356  \n",
       "3   0.0         1.0      1.000000        33.830356  \n",
       "4   0.0         1.0      1.000000        33.830356  \n",
       "..  ...         ...           ...              ...  \n",
       "52  0.0         1.0      0.023457         0.793566  \n",
       "53  0.0         1.0      0.023457         0.793566  \n",
       "54  0.0         1.0      0.023457         0.793566  \n",
       "55  0.0         1.0      0.023457         0.793566  \n",
       "56  0.0         1.0      0.023457         0.793566  \n",
       "\n",
       "[570473 rows x 16 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and normalize the training weight\n",
    "for tag in df.tag.unique():\n",
    "    # training weight *= Nevents / sum of event weight\n",
    "    df.loc[df['tag']==tag,'training_weight'] *= df.shape[0] / df[df['tag']==tag]['event_weight'].sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed89ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkBatches(df, label_column, batch_size=128, weight_column='weight'):\n",
    "    N_checks = 20\n",
    "    labels = pd.unique(df[label_column])\n",
    "    sum_label = {label:0. for label in labels}\n",
    "    N_label = {label:0 for label in labels}\n",
    "    for i in range(N_checks):\n",
    "        rnd_df = df.sample(batch_size)\n",
    "        for label in labels:\n",
    "            sum_label[label] += rnd_df[rnd_df[label_column]==label][weight_column].sum()\n",
    "            N_label[label] += rnd_df[rnd_df[label_column]==label][weight_column].shape[0]\n",
    "\n",
    "    print (f'On average, per batch the total weight is')\n",
    "    for label in labels:\n",
    "        sum_label[label] /= N_checks\n",
    "        N_label[label] /= N_checks\n",
    "        print (f'\\t... {label:20s}: {sum_label[label]:15.9f} [{N_label[label]} events]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7d873e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using event weight\n",
      "On average, per batch the total weight is\n",
      "\t... background          :     7.163721956 [252.55 events]\n",
      "\t... HH                  :     0.000697431 [3.45 events]\n",
      "Using training weight\n",
      "On average, per batch the total weight is\n",
      "\t... background          :   256.226775546 [252.4 events]\n",
      "\t... HH                  :   265.410017769 [3.6 events]\n"
     ]
    }
   ],
   "source": [
    "print ('Using event weight')\n",
    "checkBatches(df, label_column='tag', weight_column='event_weight', batch_size=parameters['batch_size'])\n",
    "print ('Using training weight')\n",
    "checkBatches(df, label_column='tag', weight_column='training_weight', batch_size=parameters['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "259e26d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the background and signal weights #\n",
    "matplotlib.use('Agg') # to avoid \"\"\"qt.qpa.plugin: Could not find the Qt platform plugin \"xcb\" in \"\" \"\"\" error\n",
    "fig,axs = plt.subplots(figsize=(25,10),nrows=len(tags),ncols=2)\n",
    "fig.subplots_adjust(left=0.1, right=0.9, top=0.98, bottom=0.1, wspace=0.2,hspace=0.4)\n",
    "for irow,tag in enumerate(tags):\n",
    "    for icol,column in enumerate(['event_weight','training_weight']):\n",
    "        axs[irow,icol].hist(df[df['tag']==tag][column],bins=100,color='b')\n",
    "        axs[irow,icol].set_title(f\"Category = {tag}\")\n",
    "        axs[irow,icol].set_xlabel(column)\n",
    "        axs[irow,icol].set_yscale('log')\n",
    "fig.savefig(\"event_weights_A.pdf\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24aafcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even set has     285451 events [50.04%]\n",
      "Odd  set has     285022 events [49.96%]\n"
     ]
    }
   ],
   "source": [
    "# Determine splitting variable #\n",
    "split_var = np.abs(df['leadingLepton_phi'].copy())\n",
    "split_var *= 1e5\n",
    "split_var -= np.floor(split_var)\n",
    "split_var = (split_var*1e1).astype(int)\n",
    "split_var = split_var %2 == 0\n",
    "print (f'Even set has {df[split_var].shape[0]:10d} events [{df[split_var].shape[0]/df.shape[0]*100:5.2f}%]')\n",
    "print (f'Odd  set has {df[~split_var].shape[0]:10d} events [{df[~split_var].shape[0]/df.shape[0]*100:5.2f}%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85f60222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using split type odd\n"
     ]
    }
   ],
   "source": [
    "# Sets splitting #\n",
    "print (f'Using split type {split}')\n",
    "if split == 'even':\n",
    "    train_df = df[~split_var] # Trained on odd\n",
    "    test_df  = df[split_var]  # Evaluated on even \n",
    "elif split == 'odd':\n",
    "    train_df = df[split_var]  # Trained on even\n",
    "    test_df  = df[~split_var] # Evaluated on odd \n",
    "else:\n",
    "    raise RuntimeError(f'Split needs to be either odd or even, is {split}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c1ced49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize for training\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edb8dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quantile corrections #\n",
    "# # When an event has a large weight, it can imbalance a lot the training, still the weight might have a meaning\n",
    "# # Idea : instead of 1 event with wi>>1, we use N copies of the event with wf = wi/N\n",
    "# # From the point of view of the physics it does not matter, the total event weight sum of each process is the same\n",
    "# # From the point of view of the DNN, we have split a tough nut to crack into several smaller ones\n",
    "\n",
    "# quantile_lim = train_df['training_weight'].quantile(0.99)\n",
    "# print (f'{(1-quantile)*100:2.2f}% right quantile is when weight is at {quantile_lim}')\n",
    "# print ('  -> These events will be repeated and their learning weights reduced accordingly to avoid unstability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ea1e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final sets\n",
      "Training set   = 199815 (35.03%)\n",
      "Validation set = 85636 (15.01%)\n",
      "Testing set    = 285022 (49.96%)\n",
      "Total set      = 570473\n"
     ]
    }
   ],
   "source": [
    "# # Select the events #\n",
    "# idx_to_repeat = train_df['training_weight'] >= quantile_lim\n",
    "# events_excess = train_df[idx_to_repeat].copy()\n",
    "# saved_columns = train_df[['training_weight','process']].copy()\n",
    "# # Compute multiplicative factor #\n",
    "# factor = (events_excess['training_weight']/quantile_lim).values.astype(np.int32)\n",
    "# # Correct the weights of events already in df #\n",
    "# train_df.loc[idx_to_repeat,'training_weight'] /= factor\n",
    "# # Add N-1 copies #\n",
    "# arr_to_repeat = train_df[idx_to_repeat].values\n",
    "# repetition = np.repeat(np.arange(arr_to_repeat.shape[0]), factor-1)\n",
    "# df_repeated = pd.DataFrame(np.take(arr_to_repeat,repetition,axis=0),columns=train_df.columns)\n",
    "# df_repeated = df_repeated.astype(train_df.dtypes.to_dict()) # otherwise dtypes are object\n",
    "# train_df = pd.concat((train_df,df_repeated),axis=0,ignore_index=True).sample(frac=1).reset_index() # Add and randomize\n",
    "# # Printout #\n",
    "# print ('Changes per process in training set')\n",
    "# for process in pd.unique(train_df['process']):\n",
    "#     N_before = saved_columns[saved_columns['process']==process].shape[0]\n",
    "#     N_after  = train_df[train_df['process']==process].shape[0]\n",
    "#     if N_before != N_after:\n",
    "#         print (f\"{process:20s}\")\n",
    "#         print (f\"... {N_before:6d} events [sum weight = {saved_columns[saved_columns['process']==process]['training_weight'].sum():14.6f}]\",end=' -> ')\n",
    "#         print (f\"{N_after:6d} events [sum weight = {train_df[train_df['process']==process]['training_weight'].sum():14.6f}]\")\n",
    "# print (f\"Total entries : {saved_columns.shape[0]:14d} -> {train_df.shape[0]:14d}\")\n",
    "# print (f\"Total event sum : {saved_columns['training_weight'].sum():14.6f} -> {train_df['training_weight'].sum():14.6f}\")\n",
    "\n",
    "# Validation split #\n",
    "train_df,val_df  = train_test_split(train_df,test_size=0.3)\n",
    "\n",
    "# Printout #\n",
    "print ('\\nFinal sets')\n",
    "print (f'Training set   = {train_df.shape[0]} ({train_df.shape[0] / df.shape[0] * 100 :2.2f}%)')\n",
    "print (f'Validation set = {val_df.shape[0]} ({val_df.shape[0] / df.shape[0] * 100 :2.2f}%)')\n",
    "print (f'Testing set    = {test_df.shape[0]} ({test_df.shape[0] / df.shape[0] * 100 :2.2f}%)')\n",
    "print (f'Total set      = {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76336e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the background and signal weights #\n",
    "# fig,axs = plt.subplots(figsize=(16,8),nrows=1,ncols=2)\n",
    "# fig.subplots_adjust(left=0.1, right=0.9, top=0.96, bottom=0.1, wspace=0.2,hspace=0.3)\n",
    "\n",
    "# if split == 'even':\n",
    "#     axs[0].hist(df[~split_var]['training_weight'],bins=100,color='b')\n",
    "# elif split == 'odd':\n",
    "#     axs[0].hist(df[split_var]['training_weight'],bins=100,color='b')\n",
    "# axs[0].set_title(\"Before correction\")\n",
    "# axs[0].set_xlabel(\"Training weight\")\n",
    "# axs[0].set_yscale('log')\n",
    "# axs[1].hist(train_df['training_weight'],bins=100,color='b')\n",
    "# axs[1].set_title(\"After correction\")\n",
    "# axs[1].set_xlabel(\"Training weight\")\n",
    "# axs[1].set_yscale('log')\n",
    "# fig.savefig(\"event_weights_C.pdf\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bd6ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 09:13:04.772247: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/sft.cern.ch/lcg/releases/MCGenerators/thepeg/2.2.3-18d03/x86_64-centos7-gcc11-opt/lib/ThePEG:/cvmfs/sft.cern.ch/lcg/releases/MCGenerators/herwig++/7.2.3-94809/x86_64-centos7-gcc11-opt/lib/Herwig:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/jaxlib/mlir/_mlir_libs:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/torch/lib:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/onnxruntime/capi/:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/contrib/tensor_forest:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/tensorflow/python/framework:/cvmfs/sft.cern.ch/lcg/releases/java/8u312-80070/x86_64-centos7-gcc11-opt/jre/lib/amd64:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib64:/cvmfs/sft.cern.ch/lcg/views/LCG_102/x86_64-centos7-gcc11-opt/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib:/cvmfs/sft.cern.ch/lcg/releases/gcc/11.2.0-8a51a/x86_64-centos7/lib64:/cvmfs/sft.cern.ch/lcg/releases/binutils/2.37-355ed/x86_64-centos7/lib:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/lib64:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/lib:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib64:/cvmfs/grid.cern.ch/centos7-umd4-ui-4.0.3-1_191004/usr/lib:/cvmfs/sft.cern.ch/lcg/releases/R/4.1.2-234e4/x86_64-centos7-gcc11-opt/lib64/R/library/readr/rcon\n",
      "2023-09-12 09:13:04.772302: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-09-12 09:13:04.772337: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ingrid-ui1.cism.ucl.ac.be): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "# Input layer #\n",
    "inputs = keras.Input(shape=(len(input_vars),), name=\"particles\")\n",
    "# Preprocessing layer\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "normalizer = preprocessing.Normalization(mean     = train_df[input_vars].mean(axis=0),\n",
    "                                        variance = train_df[input_vars].var(axis=0),\n",
    "                                        name     = 'Normalization')(inputs)\n",
    "    # this layer does the preprocessing (x-mu)/std for each input\n",
    "# Dense (hidden) layers #\n",
    "x = normalizer\n",
    "for i in range(parameters['n_layers']):\n",
    "    x = layers.Dense(units                = parameters['n_neurons'], \n",
    "                    activation           = parameters['hidden_activation'], \n",
    "                    activity_regularizer = tf.keras.regularizers.l2(parameters['l2']),\n",
    "                    name                 = f\"dense_{i}\")(x)\n",
    "    if parameters['batch_norm']:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    if parameters['dropout'] > 0.:\n",
    "        x = layers.Dropout(parameters['dropout'])(x)\n",
    "# Output layer #\n",
    "outputs = layers.Dense(units                = 2, \n",
    "                    activation           = parameters['output_activation'],\n",
    "                    activity_regularizer = tf.keras.regularizers.l2(parameters['l2']),\n",
    "                    name                 = \"predictions\")(x)\n",
    "\n",
    "# Registering the model #\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85c2f07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input (after normalization) mean (should be close to 0)\n",
      "[-3.4735160e-07  8.5373220e-10 -1.4031396e-08  4.3296929e-08\n",
      "  8.2974889e-09 -6.2445946e-09  5.0761567e-08 -4.2388310e-10\n",
      " -9.6416253e-09]\n",
      "Input (after normalization) variance (should be close to 1)\n",
      "[0.9999448  0.9999526  0.99997693 0.9999505  0.99995786 0.99996614\n",
      " 0.99995214 0.99994534 0.99997663]\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " particles (InputLayer)      [(None, 9)]               0         \n",
      "                                                                 \n",
      " Normalization (Normalizatio  (None, 9)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " dense_0 (Dense)             (None, 64)                640       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 64)               256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,858\n",
      "Trainable params: 9,474\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_preprocess = keras.Model(inputs=inputs, outputs=normalizer)\n",
    "out_test = model_preprocess.predict(train_df[input_vars],batch_size=5000)\n",
    "print ('Input (after normalization) mean (should be close to 0)')\n",
    "print (out_test.mean(axis=0))\n",
    "print ('Input (after normalization) variance (should be close to 1)')\n",
    "print (out_test.var(axis=0))\n",
    "\n",
    "model.compile(\n",
    "    #optimizer=keras.optimizers.RMSprop(),\n",
    "    optimizer='adam',  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.BinaryAccuracy(),\n",
    "            tf.keras.metrics.AUC(),\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()],\n",
    "    weighted_metrics=[]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e5028e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks #\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                            min_delta = 0.001, \n",
    "                            patience = 20,\n",
    "                            verbose=1,\n",
    "                            mode='min',\n",
    "                            restore_best_weights=True)\n",
    "# Stop the learning when val_loss stops increasing \n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "\n",
    "reduce_plateau = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                                factor = 0.1,\n",
    "                                min_delta = 0.001, \n",
    "                                patience = 8,\n",
    "                                min_lr = 1e-8,\n",
    "                                verbose=2,\n",
    "                                mode='min')\n",
    "# reduce LR if not improvement for some time \n",
    "# https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "import History \n",
    "importlib.reload(History)\n",
    "loss_history = History.LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10528faa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "781/781 - 45s - loss: 1.1096 - binary_accuracy: 0.6676 - auc: 0.7519 - precision: 0.6676 - recall: 0.6676 - val_loss: 0.9815 - val_binary_accuracy: 0.7302 - val_auc: 0.8326 - val_precision: 0.7302 - val_recall: 0.7302 - lr: 0.0010 - 45s/epoch - 58ms/step\n",
      "Epoch 2/200\n",
      "781/781 - 5s - loss: 0.8867 - binary_accuracy: 0.7621 - auc: 0.8640 - precision: 0.7621 - recall: 0.7621 - val_loss: 0.8598 - val_binary_accuracy: 0.7872 - val_auc: 0.8893 - val_precision: 0.7872 - val_recall: 0.7872 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
      "Epoch 3/200\n",
      "781/781 - 4s - loss: 0.8279 - binary_accuracy: 0.7767 - auc: 0.8841 - precision: 0.7767 - recall: 0.7767 - val_loss: 0.8845 - val_binary_accuracy: 0.7850 - val_auc: 0.8898 - val_precision: 0.7850 - val_recall: 0.7850 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 4/200\n",
      "781/781 - 4s - loss: 0.7999 - binary_accuracy: 0.7798 - auc: 0.8896 - precision: 0.7798 - recall: 0.7798 - val_loss: 0.8551 - val_binary_accuracy: 0.7866 - val_auc: 0.8972 - val_precision: 0.7866 - val_recall: 0.7866 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 5/200\n",
      "781/781 - 4s - loss: 0.7810 - binary_accuracy: 0.7821 - auc: 0.8941 - precision: 0.7821 - recall: 0.7821 - val_loss: 0.8620 - val_binary_accuracy: 0.7561 - val_auc: 0.8715 - val_precision: 0.7561 - val_recall: 0.7561 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 6/200\n",
      "781/781 - 4s - loss: 0.7686 - binary_accuracy: 0.7870 - auc: 0.8983 - precision: 0.7870 - recall: 0.7870 - val_loss: 0.8289 - val_binary_accuracy: 0.7763 - val_auc: 0.8953 - val_precision: 0.7763 - val_recall: 0.7763 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 7/200\n",
      "781/781 - 4s - loss: 0.7481 - binary_accuracy: 0.7918 - auc: 0.9031 - precision: 0.7918 - recall: 0.7918 - val_loss: 0.8103 - val_binary_accuracy: 0.7819 - val_auc: 0.8991 - val_precision: 0.7819 - val_recall: 0.7819 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 8/200\n",
      "781/781 - 5s - loss: 0.7471 - binary_accuracy: 0.7890 - auc: 0.9028 - precision: 0.7890 - recall: 0.7890 - val_loss: 0.8479 - val_binary_accuracy: 0.7637 - val_auc: 0.8802 - val_precision: 0.7637 - val_recall: 0.7637 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
      "Epoch 9/200\n",
      "781/781 - 5s - loss: 0.7396 - binary_accuracy: 0.7930 - auc: 0.9055 - precision: 0.7930 - recall: 0.7930 - val_loss: 0.8098 - val_binary_accuracy: 0.7899 - val_auc: 0.9050 - val_precision: 0.7899 - val_recall: 0.7899 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
      "Epoch 10/200\n",
      "781/781 - 4s - loss: 0.7290 - binary_accuracy: 0.7967 - auc: 0.9083 - precision: 0.7967 - recall: 0.7967 - val_loss: 0.8260 - val_binary_accuracy: 0.7886 - val_auc: 0.9022 - val_precision: 0.7886 - val_recall: 0.7886 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 11/200\n",
      "781/781 - 4s - loss: 0.7223 - binary_accuracy: 0.7951 - auc: 0.9078 - precision: 0.7951 - recall: 0.7951 - val_loss: 0.8294 - val_binary_accuracy: 0.7654 - val_auc: 0.8929 - val_precision: 0.7654 - val_recall: 0.7654 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 12/200\n",
      "781/781 - 4s - loss: 0.7147 - binary_accuracy: 0.7932 - auc: 0.9080 - precision: 0.7932 - recall: 0.7932 - val_loss: 0.8563 - val_binary_accuracy: 0.7977 - val_auc: 0.9090 - val_precision: 0.7977 - val_recall: 0.7977 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 13/200\n",
      "781/781 - 5s - loss: 0.7052 - binary_accuracy: 0.7949 - auc: 0.9086 - precision: 0.7949 - recall: 0.7949 - val_loss: 0.8585 - val_binary_accuracy: 0.8142 - val_auc: 0.9230 - val_precision: 0.8142 - val_recall: 0.8142 - lr: 0.0010 - 5s/epoch - 6ms/step\n",
      "Epoch 14/200\n",
      "781/781 - 4s - loss: 0.6944 - binary_accuracy: 0.8010 - auc: 0.9133 - precision: 0.8010 - recall: 0.8010 - val_loss: 0.8410 - val_binary_accuracy: 0.8071 - val_auc: 0.9150 - val_precision: 0.8071 - val_recall: 0.8071 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "781/781 - 4s - loss: 0.6914 - binary_accuracy: 0.7993 - auc: 0.9121 - precision: 0.7993 - recall: 0.7993 - val_loss: 0.8638 - val_binary_accuracy: 0.7671 - val_auc: 0.8912 - val_precision: 0.7671 - val_recall: 0.7671 - lr: 0.0010 - 4s/epoch - 6ms/step\n",
      "Epoch 16/200\n",
      "781/781 - 4s - loss: 0.6352 - binary_accuracy: 0.8005 - auc: 0.9145 - precision: 0.8005 - recall: 0.8005 - val_loss: 0.8138 - val_binary_accuracy: 0.7963 - val_auc: 0.9114 - val_precision: 0.7963 - val_recall: 0.7963 - lr: 1.0000e-04 - 4s/epoch - 6ms/step\n",
      "Epoch 17/200\n",
      "781/781 - 5s - loss: 0.6082 - binary_accuracy: 0.8095 - auc: 0.9208 - precision: 0.8095 - recall: 0.8095 - val_loss: 0.8198 - val_binary_accuracy: 0.8114 - val_auc: 0.9214 - val_precision: 0.8114 - val_recall: 0.8114 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
      "Epoch 18/200\n",
      "781/781 - 5s - loss: 0.6036 - binary_accuracy: 0.8153 - auc: 0.9243 - precision: 0.8153 - recall: 0.8153 - val_loss: 0.8269 - val_binary_accuracy: 0.8122 - val_auc: 0.9222 - val_precision: 0.8122 - val_recall: 0.8122 - lr: 1.0000e-04 - 5s/epoch - 6ms/step\n",
      "Epoch 19/200\n",
      "781/781 - 4s - loss: 0.5986 - binary_accuracy: 0.8167 - auc: 0.9251 - precision: 0.8167 - recall: 0.8167 - val_loss: 0.8273 - val_binary_accuracy: 0.8087 - val_auc: 0.9199 - val_precision: 0.8087 - val_recall: 0.8087 - lr: 1.0000e-04 - 4s/epoch - 6ms/step\n",
      "Epoch 20/200\n",
      "781/781 - 4s - loss: 0.5951 - binary_accuracy: 0.8176 - auc: 0.9262 - precision: 0.8176 - recall: 0.8176 - val_loss: 0.8374 - val_binary_accuracy: 0.8118 - val_auc: 0.9220 - val_precision: 0.8118 - val_recall: 0.8118 - lr: 1.0000e-04 - 4s/epoch - 6ms/step\n",
      "Epoch 21/200\n",
      "781/781 - 4s - loss: 0.5902 - binary_accuracy: 0.8179 - auc: 0.9265 - precision: 0.8179 - recall: 0.8179 - val_loss: 0.8419 - val_binary_accuracy: 0.8124 - val_auc: 0.9226 - val_precision: 0.8124 - val_recall: 0.8124 - lr: 1.0000e-04 - 4s/epoch - 6ms/step\n",
      "Epoch 22/200\n",
      "781/781 - 4s - loss: 0.5833 - binary_accuracy: 0.8193 - auc: 0.9274 - precision: 0.8193 - recall: 0.8193 - val_loss: 0.8505 - val_binary_accuracy: 0.8173 - val_auc: 0.9259 - val_precision: 0.8173 - val_recall: 0.8173 - lr: 1.0000e-04 - 4s/epoch - 6ms/step\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "781/781 - 4s - loss: 0.5809 - binary_accuracy: 0.8213 - auc: 0.9284 - precision: 0.8213 - recall: 0.8213 - val_loss: 0.8463 - val_binary_accuracy: 0.8062 - val_auc: 0.9189 - val_precision: 0.8062 - val_recall: 0.8062 - lr: 1.0000e-04 - 4s/epoch - 5ms/step\n",
      "Epoch 24/200\n",
      "781/781 - 4s - loss: 0.5687 - binary_accuracy: 0.8165 - auc: 0.9256 - precision: 0.8165 - recall: 0.8165 - val_loss: 0.8481 - val_binary_accuracy: 0.8116 - val_auc: 0.9228 - val_precision: 0.8116 - val_recall: 0.8116 - lr: 1.0000e-05 - 4s/epoch - 6ms/step\n",
      "Epoch 25/200\n",
      "781/781 - 4s - loss: 0.5691 - binary_accuracy: 0.8182 - auc: 0.9272 - precision: 0.8182 - recall: 0.8182 - val_loss: 0.8501 - val_binary_accuracy: 0.8158 - val_auc: 0.9255 - val_precision: 0.8158 - val_recall: 0.8158 - lr: 1.0000e-05 - 4s/epoch - 6ms/step\n",
      "Epoch 26/200\n",
      "781/781 - 4s - loss: 0.5717 - binary_accuracy: 0.8199 - auc: 0.9277 - precision: 0.8199 - recall: 0.8199 - val_loss: 0.8495 - val_binary_accuracy: 0.8127 - val_auc: 0.9235 - val_precision: 0.8127 - val_recall: 0.8127 - lr: 1.0000e-05 - 4s/epoch - 5ms/step\n",
      "Epoch 27/200\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "781/781 - 4s - loss: 0.5618 - binary_accuracy: 0.8222 - auc: 0.9291 - precision: 0.8222 - recall: 0.8222 - val_loss: 0.8518 - val_binary_accuracy: 0.8173 - val_auc: 0.9264 - val_precision: 0.8173 - val_recall: 0.8173 - lr: 1.0000e-05 - 4s/epoch - 5ms/step\n",
      "Epoch 27: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_df[input_vars],\n",
    "    train_df[tags],\n",
    "    verbose=2,\n",
    "    batch_size=parameters['batch_size'],\n",
    "    epochs=parameters['epochs'],\n",
    "    sample_weight=train_df['training_weight'],\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(val_df[input_vars],val_df[tags],val_df['training_weight']),\n",
    "    callbacks = [early_stopping, reduce_plateau, loss_history],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0f5ca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curves saved as loss_test1_odd.png\n",
      "printed history\n"
     ]
    }
   ],
   "source": [
    "History.PlotHistory(loss_history,params=parameters,outputName=f'loss_{suffix}_{split}.png') # giving an error i.e.\n",
    "# Params is a dict of parameters with name and values\n",
    "# used for plotting\n",
    "print(\"printed history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdaee362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce output on the test set as new column #\n",
    "output = model.predict(test_df[input_vars],batch_size=5000)\n",
    "\n",
    "output_tags = [f'output {tag}' for tag in tags]\n",
    "    # Here the batch_size arg is independent of the learning\n",
    "    # Default is 32, but it can become slow, by using large value it will just compute more values in parallel\n",
    "    # (more or less parallel, we are not using a GPU)\n",
    "for output_tag in output_tags:\n",
    "    if output_tag in test_df.columns:\n",
    "        # If already output, need to remove to add again\n",
    "        # avoid issues in case you run this cell multiple times\n",
    "        del test_df[output_tag]\n",
    "\n",
    "test_df = pd.concat((test_df,pd.DataFrame(output,columns=output_tags,index=test_df.index)),axis=1)\n",
    "# We add the output as a column, a bit messy, different ways, here I use a concatenation\n",
    "\n",
    "# Make the discriminator #\n",
    "if 'd_HH' in test_df.columns:\n",
    "    del test_df['d_HH']\n",
    "    \n",
    "signal_idx = [i for i,tag in enumerate(tags) if 'HH' in tag]\n",
    "\n",
    "# d_HH = ln (P(HH) / (P(single H) + P(background)))\n",
    "\n",
    "#test_df['d_HH'] = pd.Series(np.ones(test_df.shape[0]))\n",
    "\n",
    "# Numerator #\n",
    "num = pd.DataFrame((test_df[[output_tags[i] for i in range(len(tags)) if i in signal_idx]]).sum(axis=1))\n",
    "# Denominator #\n",
    "den = pd.DataFrame(test_df[[output_tags[i] for i in range(len(tags)) if i not in signal_idx]].sum(axis=1))\n",
    "# Ln #\n",
    "d_HH = np.log(num / den)\n",
    "test_df['d_HH'] = d_HH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb051e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC curves\n",
      "ROC curve of binary classification of HH node versus all the others\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/home/users/a/g/aguzel/bambooML/HHtoWWbb_Run3/python/roc.py:80: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC as roc_test1_odd_HH.pdf\n",
      "Best WP based on significance = 0.91670\n",
      "ROC curve of binary classification of background node versus all the others\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/home/users/a/g/aguzel/bambooML/HHtoWWbb_Run3/python/roc.py:80: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC as roc_test1_odd_background.pdf\n",
      "ROC curve of binary classification of d_HH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/home/users/a/g/aguzel/bambooML/HHtoWWbb_Run3/python/roc.py:80: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC as roc_test1_odd_d_HH.pdf\n",
      "Best WP based on significance = 2.40007\n",
      "Multi roc curve for `output HH`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/home/users/a/g/aguzel/bambooML/HHtoWWbb_Run3/python/roc.py:110: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved multi ROC as multi_roc_test1_odd_output_HH.pdf\n",
      "Multi roc curve for `output background`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/auto/home/users/a/g/aguzel/bambooML/HHtoWWbb_Run3/python/roc.py:110: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved multi ROC as multi_roc_test1_odd_output_background.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"ROC curves\")\n",
    "import roc\n",
    "importlib.reload(roc) # Reload in case file has changed\n",
    "for tag in tags:\n",
    "    print (f'ROC curve of binary classification of {tag} node versus all the others')\n",
    "    roc.rocAndSig(y_true                 = test_df[tag],\n",
    "                y_pred                 = test_df[f'output {tag}'],\n",
    "                w_roc                  = test_df['training_weight'],\n",
    "                w_sig                  = test_df['event_weight'],\n",
    "                show_significance      = 'HH' in tag,\n",
    "                outputName             = f'roc_{suffix}_{split}_{tag}.pdf')\n",
    "\n",
    "# Multiclassification ROC curves are a bit harder to interpret than binary classification\n",
    "# Here I do one versus the rest, so each ROC curves shows how the DNN is able to classify\n",
    "# one class (HH, single H or background) versus all the others, which is one projection on\n",
    "# how to see the performances\n",
    "# For HH I show the significance but more as an information, because using only the HH node \n",
    "# means we do not use all the power of the multiclass (-> d_HH is for that)\n",
    "print (f'ROC curve of binary classification of d_HH')\n",
    "roc.rocAndSig(y_true                 = test_df['HH'],\n",
    "            y_pred                 = test_df['d_HH'],\n",
    "            w_roc                  = test_df['training_weight'],\n",
    "            w_sig                  = test_df['event_weight'],\n",
    "            show_significance      = True,\n",
    "            outputName             = f'roc_{suffix}_{split}_d_HH.pdf')\n",
    "\n",
    "# Tryign a new things, seeing the discrimination power of each node, class wise\n",
    "for tag in tags:\n",
    "    print (f'Multi roc curve for `output {tag}`')\n",
    "    tags_order = [tag] + [t for t in tags if t != tag]\n",
    "    roc.multiRoc(outputs    = [test_df[test_df['tag']==tag][f'output {tag}'] for tag in tags_order],\n",
    "                tags       = tags_order,\n",
    "                weights    = [test_df[test_df['tag']==tag]['training_weight'] for tag in tags_order],\n",
    "                title      = f'Using node {tag}',\n",
    "                outputName = f'multi_roc_{suffix}_{split}_output_{tag}.pdf')\n",
    "    \n",
    "fig,axs = plt.subplots(figsize=(6,12),nrows=len(tags)+1,ncols=2)\n",
    "fig.subplots_adjust(left=0.1, right=0.9, top=0.98, bottom=0.1, wspace=0.3,hspace=0.5)\n",
    "\n",
    "tag_df = {tag:test_df[test_df['tag']==tag] for tag in tags}\n",
    "colors = ['g','r','b']\n",
    "\n",
    "# Manual binning so we can compute significance #\n",
    "n_bins = 50\n",
    "\n",
    "def get_bin_content(bins,y,w):\n",
    "    digitized = np.digitize(y,bins)\n",
    "    return np.array([w[digitized==i].sum() for i in range(1, len(bins))])\n",
    "\n",
    "for irow,output_tag in enumerate(output_tags+['d_HH']):\n",
    "    for icol,weight in enumerate(['event_weight','training_weight']):\n",
    "        # Fill the bins myself #\n",
    "        bins = np.linspace(test_df[output_tag].min(),test_df[output_tag].max(),n_bins+1)\n",
    "        centers = (bins[1:]+bins[:-1])/2\n",
    "        widths = np.diff(bins)\n",
    "        \n",
    "        tag_content = {tag:get_bin_content(bins,tag_df[tag][output_tag],tag_df[tag][weight])for tag in tags}\n",
    "        tag_cumsum_left = {tag:np.cumsum(tag_content[tag])/tag_content[tag].sum() for tag in tags}\n",
    "        tag_cumsum_right = {tag:np.cumsum(tag_content[tag][::-1])[::-1]/tag_content[tag].sum() for tag in tags}\n",
    "        # Need to integrate all the bins right of the DNN cut to get significance\n",
    "        #z_left = np.nan_to_num(np.sqrt(2*((cumsum_s_left+cumsum_b_left)*np.log(1+cumsum_s_left/cumsum_b_left)-cumsum_s_left)))\n",
    "        #z_right = np.nan_to_num(np.sqrt(2*((cumsum_s_right+cumsum_b_right)*np.log(1+cumsum_s_right/cumsum_b_right)-cumsum_s_right)))\n",
    "        #z_left /= z_left.max()\n",
    "        #z_right /= z_right.max()\n",
    "        for i,(tag,content) in enumerate(tag_content.items()):\n",
    "            axs[irow,icol].bar(x=centers,height=content,width=widths,fill=False,edgecolor=colors[i],label=tag)     \n",
    "        #ax2=axs[irow,icol].twinx()   \n",
    "        \n",
    "        #ax2.plot(centers,z_left,color='r',label='Significance (left of cut) [normed]')\n",
    "        #ax2.plot(centers,z_right,color='r',linestyle='--',label='Significance (right of cut) [normed]')\n",
    "\n",
    "        #for i,tag in enumerate(tag_content.keys()):\n",
    "        #    ax2.plot(centers,content,color=colors[i],linestyle='-',label=f'{tag} content (left of cut)')\n",
    "        #    ax2.plot(centers,color=colors[i],linestyle='--',label=f'{tag} content (right of cut)')\n",
    "        \n",
    "        #ax2.set_yscale(\"log\")\n",
    "        #ax2.set_ylim([0,1.4])\n",
    "        #ax2.set_ylabel('Cumulative distribution')\n",
    "        #ax2.legend(loc='upper right')\n",
    "\n",
    "        axs[irow,icol].set_title(f\"Using {weight}\")\n",
    "        axs[irow,icol].set_xlabel(output_tag)\n",
    "        axs[irow,icol].set_ylabel('Yield')\n",
    "        axs[irow,icol].set_ylim(1e-5,max([content.max() for content in tag_content.values()])*100)\n",
    "        axs[irow,icol].set_yscale('log')\n",
    "        axs[irow,icol].legend(loc='upper left')\n",
    "fig.savefig(f\"prediction_{suffix}_{split}.pdf\", dpi = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea08ded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "print(\"evaluate the model...\")\n",
    "scores = model.evaluate(test_df[input_vars], \n",
    "                        test_df[tags], \n",
    "                        sample_weight = test_df['training_weight'], \n",
    "                        batch_size = 5000,\n",
    "                        verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea382f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save model and architecture to a single file\n",
    "modelName = f\"model_{suffix}_{split}\"\n",
    "model.save(modelName)\n",
    "print(f\"Saved model to disk as {modelName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c2dbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62d6f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
