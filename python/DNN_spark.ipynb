{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22512f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optparse\n",
    "import yaml\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5398a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage = 'usage: %prog [options]'\n",
    "# parser = optparse.OptionParser(usage)\n",
    "\n",
    "# parser.add_option('-o', '--outDir',\n",
    "#                     dest='outputPath',\n",
    "#                     help='output directory',\n",
    "#                     type='string')\n",
    "\n",
    "# (opt, args) = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d7a4d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 10 input variables\n"
     ]
    }
   ],
   "source": [
    "### Parameters of the training ###\n",
    "\n",
    "#split = \"even\" \n",
    "split = \"odd\" \n",
    "# split = even | odd -> on what split to train the model (will be in the name)\n",
    "# -> you need one \"odd\" and one \"even\" models to be put inside bamboo\n",
    "\n",
    "suffix = 'test1'\n",
    "# Suffix that will be added to the saved model (so multiple DNNs can be trained)\n",
    "\n",
    "quantile = 1.0 # We will repeat the part of the weights rightmost tail\n",
    "# Eg : 0.95, means we take the 5% events on the right tail of training weight and repeat them\n",
    "# 1.0 means no correction (to be used if you want to disable it)\n",
    "\n",
    "tags = ['HH','background']\n",
    "\n",
    "# DNN hyperparameters #\n",
    "parameters = {\n",
    "    'epochs'                : 200,\n",
    "    'lr'                    : 0.001,\n",
    "    'batch_size'            : 256,\n",
    "    'n_layers'              : 3,\n",
    "    'n_neurons'             : 64,\n",
    "    'hidden_activation'     : 'relu',\n",
    "    'output_activation'     : 'softmax',\n",
    "    'l2'                    : 1e-6,\n",
    "    'dropout'               : 0.,\n",
    "    'batch_norm'            : True,\n",
    "}\n",
    "# Input variables\n",
    "input_vars=[\n",
    "    \"weight\",\n",
    "    \"ak4bjet1_pt\",\n",
    "    \"ak4bjet1_eta\",\n",
    "    \"ak4bjet1_phi\",\n",
    "    \"leadingLepton_pt\",\n",
    "    \"leadingLepton_eta\",\n",
    "    \"leadingLepton_phi\",\n",
    "    \"subleadingLepton_pt\",\n",
    "    \"subleadingLepton_eta\",\n",
    "    \"subleadingLepton_phi\"\n",
    "    ]\n",
    "\n",
    "print(f'Using {len(input_vars)} input variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ac770a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using skim file ../output/mvaSkim-try2-full/results/DL_resolved.parquet\n",
      "Using yaml file ../output/mvaSkim-try2-full/plots.yml\n"
     ]
    }
   ],
   "source": [
    "outputPath = '../output/mvaSkim-try2-full'\n",
    "yamlFile = os.path.join(outputPath,'plots.yml')\n",
    "skimFile = os.path.join(outputPath,'results/DL_resolved.parquet')\n",
    "# yamlFile = os.path.join(opt.outputPath,'plots.yml')\n",
    "# skimFile = os.path.join(opt.outputPath,'results/DL_resolved.parquet')\n",
    "\n",
    "print(f'Using skim file {skimFile}')\n",
    "print(f'Using yaml file {yamlFile}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "eaa691c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "100132f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe from parquet file\n",
    "df=spark.read.parquet(skimFile)\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True) # enable visual output suitable for jupyter notebook\n",
    "spark.conf.set('spark.sql.repl.eagerEval.maxNumRows', 5) # set maximum number of rows shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "abfd44f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ak4bjet1_eta</th><th>ak4bjet1_phi</th><th>ak4bjet1_pt</th><th>leadingLepton_eta</th><th>leadingLepton_phi</th><th>leadingLepton_pt</th><th>subleadingLepton_eta</th><th>subleadingLepton_phi</th><th>subleadingLepton_pt</th><th>weight</th><th>process</th><th>__index_level_0__</th></tr>\n",
       "<tr><td>-1.4255371</td><td>-1.3234863</td><td>126.1875</td><td>-0.1618042</td><td>0.8876953</td><td>28.389597</td><td>-0.29522705</td><td>-1.3688965</td><td>19.328093</td><td>1.0</td><td>data</td><td>0</td></tr>\n",
       "<tr><td>1.1962891</td><td>-0.49829102</td><td>48.59375</td><td>0.31628418</td><td>-2.8691406</td><td>109.99773</td><td>-0.36401367</td><td>-0.5653076</td><td>38.17057</td><td>1.0</td><td>data</td><td>1</td></tr>\n",
       "<tr><td>1.0664062</td><td>-1.6318359</td><td>59.71875</td><td>-0.13739014</td><td>-2.4487305</td><td>66.637535</td><td>0.11885071</td><td>-1.0979004</td><td>41.26601</td><td>1.0</td><td>data</td><td>2</td></tr>\n",
       "<tr><td>-1.0935059</td><td>1.3464355</td><td>96.0</td><td>0.02368164</td><td>-0.66955566</td><td>51.262684</td><td>0.5213623</td><td>2.9233398</td><td>50.410046</td><td>1.0</td><td>data</td><td>3</td></tr>\n",
       "<tr><td>1.2924805</td><td>1.2346191</td><td>50.28125</td><td>0.5716553</td><td>2.4506836</td><td>202.46753</td><td>1.092041</td><td>-0.9649658</td><td>44.80811</td><td>1.0</td><td>data</td><td>4</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+------------+------------+-----------+-----------------+-----------------+----------------+--------------------+--------------------+-------------------+------+-------+-----------------+\n",
       "|ak4bjet1_eta|ak4bjet1_phi|ak4bjet1_pt|leadingLepton_eta|leadingLepton_phi|leadingLepton_pt|subleadingLepton_eta|subleadingLepton_phi|subleadingLepton_pt|weight|process|__index_level_0__|\n",
       "+------------+------------+-----------+-----------------+-----------------+----------------+--------------------+--------------------+-------------------+------+-------+-----------------+\n",
       "|  -1.4255371|  -1.3234863|   126.1875|       -0.1618042|        0.8876953|       28.389597|         -0.29522705|          -1.3688965|          19.328093|   1.0|   data|                0|\n",
       "|   1.1962891| -0.49829102|   48.59375|       0.31628418|       -2.8691406|       109.99773|         -0.36401367|          -0.5653076|           38.17057|   1.0|   data|                1|\n",
       "|   1.0664062|  -1.6318359|   59.71875|      -0.13739014|       -2.4487305|       66.637535|          0.11885071|          -1.0979004|           41.26601|   1.0|   data|                2|\n",
       "|  -1.0935059|   1.3464355|       96.0|       0.02368164|      -0.66955566|       51.262684|           0.5213623|           2.9233398|          50.410046|   1.0|   data|                3|\n",
       "|   1.2924805|   1.2346191|   50.28125|        0.5716553|        2.4506836|       202.46753|            1.092041|          -0.9649658|           44.80811|   1.0|   data|                4|\n",
       "+------------+------------+-----------+-----------------+-----------------+----------------+--------------------+--------------------+-------------------+------+-------+-----------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "be763bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " ak4bjet1_eta         | -1.4255371  \n",
      " ak4bjet1_phi         | -1.3234863  \n",
      " ak4bjet1_pt          | 126.1875    \n",
      " leadingLepton_eta    | -0.1618042  \n",
      " leadingLepton_phi    | 0.8876953   \n",
      " leadingLepton_pt     | 28.389597   \n",
      " subleadingLepton_eta | -0.29522705 \n",
      " subleadingLepton_phi | -1.3688965  \n",
      " subleadingLepton_pt  | 19.328093   \n",
      " weight               | 1.0         \n",
      " process              | data        \n",
      " __index_level_0__    | 0           \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True) # useful when rows are too long to show horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "7db14000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tag column with constant value (background) and change the tag as HH when the process is signal\n",
    "df = df.withColumn(\"tag\", F.when(df.process=='HH', 'HH').otherwise(F.lit('background')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b2dee984",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(df.select(df.tag).distinct().count() == len(tags)) # double check the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "3d0aab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "fb21a819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>HH</th></tr>\n",
       "<tr><td>0.0</td></tr>\n",
       "<tr><td>1.0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+\n",
       "| HH|\n",
       "+---+\n",
       "|0.0|\n",
       "|1.0|\n",
       "+---+"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding is a way to convert a column to a format that is easier for machine learning applications\n",
    "# Here I transform the tag (bkg or signal) column to binary and add it as new columns\n",
    "# One-hot encoding will be necessary later on\n",
    "indexer = StringIndexer(inputCol='tag', outputCol='HH')\n",
    "indexer_fitted = indexer.fit(df)\n",
    "df = indexer_fitted.transform(df)\n",
    "df.select(df.HH).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "57a0cebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events with negative weight is : 1995\n"
     ]
    }
   ],
   "source": [
    "# remove events with negative weight\n",
    "Nbefore_weight_cut = df.count()\n",
    "df = df.filter(df.weight > 0)\n",
    "Nafter_weight_cut = df.count()\n",
    "Nevents_with_negative_weight = Nbefore_weight_cut - Nafter_weight_cut\n",
    "print(f\"Number of events with negative weight is : {Nevents_with_negative_weight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "68ad58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the weight column as event weight\n",
    "df = df.withColumn(\"event_weight\", df.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "2d602215",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( df.filter(df.event_weight < 0).count() != 0 ):\n",
    "    print(f\"There are {df.filter(df.event_weight < 0).count()} events with negative weight.\")\n",
    "    print(\"/n This should be a problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "56d66284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the weight column as training weight\n",
    "df = df.withColumn(\"training_weight\", df.event_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b6928959",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ttag in tags:\n",
    "    df.filter(df.tag == ttag).training_weight *= df.count() / df.select(F.sum(df.filter(df.tag == 'HH').event_weight)).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "a7ca9875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ak4bjet1_eta</th><th>ak4bjet1_phi</th><th>ak4bjet1_pt</th><th>leadingLepton_eta</th><th>leadingLepton_phi</th><th>leadingLepton_pt</th><th>subleadingLepton_eta</th><th>subleadingLepton_phi</th><th>subleadingLepton_pt</th><th>weight</th><th>process</th><th>__index_level_0__</th><th>tag</th><th>HH</th><th>event_weight</th><th>training_weight</th></tr>\n",
       "<tr><td>-1.4255371</td><td>-1.3234863</td><td>126.1875</td><td>-0.1618042</td><td>0.8876953</td><td>28.389597</td><td>-0.29522705</td><td>-1.3688965</td><td>19.328093</td><td>1.0</td><td>data</td><td>0</td><td>background</td><td>0.0</td><td>1.0</td><td>1.0</td></tr>\n",
       "<tr><td>1.1962891</td><td>-0.49829102</td><td>48.59375</td><td>0.31628418</td><td>-2.8691406</td><td>109.99773</td><td>-0.36401367</td><td>-0.5653076</td><td>38.17057</td><td>1.0</td><td>data</td><td>1</td><td>background</td><td>0.0</td><td>1.0</td><td>1.0</td></tr>\n",
       "<tr><td>1.0664062</td><td>-1.6318359</td><td>59.71875</td><td>-0.13739014</td><td>-2.4487305</td><td>66.637535</td><td>0.11885071</td><td>-1.0979004</td><td>41.26601</td><td>1.0</td><td>data</td><td>2</td><td>background</td><td>0.0</td><td>1.0</td><td>1.0</td></tr>\n",
       "<tr><td>-1.0935059</td><td>1.3464355</td><td>96.0</td><td>0.02368164</td><td>-0.66955566</td><td>51.262684</td><td>0.5213623</td><td>2.9233398</td><td>50.410046</td><td>1.0</td><td>data</td><td>3</td><td>background</td><td>0.0</td><td>1.0</td><td>1.0</td></tr>\n",
       "<tr><td>1.2924805</td><td>1.2346191</td><td>50.28125</td><td>0.5716553</td><td>2.4506836</td><td>202.46753</td><td>1.092041</td><td>-0.9649658</td><td>44.80811</td><td>1.0</td><td>data</td><td>4</td><td>background</td><td>0.0</td><td>1.0</td><td>1.0</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+------------+------------+-----------+-----------------+-----------------+----------------+--------------------+--------------------+-------------------+------+-------+-----------------+----------+---+------------+---------------+\n",
       "|ak4bjet1_eta|ak4bjet1_phi|ak4bjet1_pt|leadingLepton_eta|leadingLepton_phi|leadingLepton_pt|subleadingLepton_eta|subleadingLepton_phi|subleadingLepton_pt|weight|process|__index_level_0__|       tag| HH|event_weight|training_weight|\n",
       "+------------+------------+-----------+-----------------+-----------------+----------------+--------------------+--------------------+-------------------+------+-------+-----------------+----------+---+------------+---------------+\n",
       "|  -1.4255371|  -1.3234863|   126.1875|       -0.1618042|        0.8876953|       28.389597|         -0.29522705|          -1.3688965|          19.328093|   1.0|   data|                0|background|0.0|         1.0|            1.0|\n",
       "|   1.1962891| -0.49829102|   48.59375|       0.31628418|       -2.8691406|       109.99773|         -0.36401367|          -0.5653076|           38.17057|   1.0|   data|                1|background|0.0|         1.0|            1.0|\n",
       "|   1.0664062|  -1.6318359|   59.71875|      -0.13739014|       -2.4487305|       66.637535|          0.11885071|          -1.0979004|           41.26601|   1.0|   data|                2|background|0.0|         1.0|            1.0|\n",
       "|  -1.0935059|   1.3464355|       96.0|       0.02368164|      -0.66955566|       51.262684|           0.5213623|           2.9233398|          50.410046|   1.0|   data|                3|background|0.0|         1.0|            1.0|\n",
       "|   1.2924805|   1.2346191|   50.28125|        0.5716553|        2.4506836|       202.46753|            1.092041|          -0.9649658|           44.80811|   1.0|   data|                4|background|0.0|         1.0|            1.0|\n",
       "+------------+------------+-----------+-----------------+-----------------+----------------+--------------------+--------------------+-------------------+------+-------+-----------------+----------+---+------------+---------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "8418aa5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ak4bjet1_eta</th><th>ak4bjet1_phi</th><th>ak4bjet1_pt</th><th>leadingLepton_eta</th><th>leadingLepton_phi</th><th>leadingLepton_pt</th><th>subleadingLepton_eta</th><th>subleadingLepton_phi</th><th>subleadingLepton_pt</th><th>weight</th><th>process</th><th>__index_level_0__</th><th>tag</th><th>HH</th><th>event_weight</th><th>training_weight</th><th>training_weight_HH</th><th>training_weight_background</th></tr>\n",
       "<tr><td>0.69104004</td><td>2.7128906</td><td>65.9375</td><td>-0.2588501</td><td>-0.49938965</td><td>67.73664</td><td>0.3043213</td><td>-0.9926758</td><td>28.382645</td><td>2.021832915488630...</td><td>HH</td><td>0</td><td>HH</td><td>1.0</td><td>2.021832915488630...</td><td>2.021832915488630...</td><td>0.006520355193267766</td><td>2.021832915488630...</td></tr>\n",
       "<tr><td>0.068725586</td><td>-3.0742188</td><td>97.0</td><td>1.6083984</td><td>0.27520752</td><td>42.937286</td><td>0.18374634</td><td>1.1877441</td><td>24.936188</td><td>2.021832915488630...</td><td>HH</td><td>1</td><td>HH</td><td>1.0</td><td>2.021832915488630...</td><td>2.021832915488630...</td><td>0.006520355193267766</td><td>2.021832915488630...</td></tr>\n",
       "<tr><td>0.756958</td><td>-0.05404663</td><td>98.875</td><td>0.48364258</td><td>2.505371</td><td>41.58864</td><td>-0.3071289</td><td>3.104004</td><td>40.117764</td><td>2.021832915488630...</td><td>HH</td><td>2</td><td>HH</td><td>1.0</td><td>2.021832915488630...</td><td>2.021832915488630...</td><td>0.006520355193267766</td><td>2.021832915488630...</td></tr>\n",
       "<tr><td>2.1328125</td><td>-2.772461</td><td>148.75</td><td>1.0141602</td><td>0.833374</td><td>77.491875</td><td>1.3835449</td><td>1.4135742</td><td>29.633177</td><td>2.021832915488630...</td><td>HH</td><td>3</td><td>HH</td><td>1.0</td><td>2.021832915488630...</td><td>2.021832915488630...</td><td>0.006520355193267766</td><td>2.021832915488630...</td></tr>\n",
       "<tr><td>-0.57177734</td><td>-0.19345093</td><td>222.25</td><td>-1.0063477</td><td>-2.1157227</td><td>87.77944</td><td>-0.58496094</td><td>-2.224121</td><td>21.291397</td><td>2.021832915488630...</td><td>HH</td><td>4</td><td>HH</td><td>1.0</td><td>2.021832915488630...</td><td>2.021832915488630...</td><td>0.006520355193267766</td><td>2.021832915488630...</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+------------+------------+-----------+-----------------+-----------------+----------------+--------------------+--------------------+-------------------+--------------------+-------+-----------------+---+---+--------------------+--------------------+--------------------+--------------------------+\n",
       "|ak4bjet1_eta|ak4bjet1_phi|ak4bjet1_pt|leadingLepton_eta|leadingLepton_phi|leadingLepton_pt|subleadingLepton_eta|subleadingLepton_phi|subleadingLepton_pt|              weight|process|__index_level_0__|tag| HH|        event_weight|     training_weight|  training_weight_HH|training_weight_background|\n",
       "+------------+------------+-----------+-----------------+-----------------+----------------+--------------------+--------------------+-------------------+--------------------+-------+-----------------+---+---+--------------------+--------------------+--------------------+--------------------------+\n",
       "|  0.69104004|   2.7128906|    65.9375|       -0.2588501|      -0.49938965|        67.73664|           0.3043213|          -0.9926758|          28.382645|2.021832915488630...|     HH|                0| HH|1.0|2.021832915488630...|2.021832915488630...|0.006520355193267766|      2.021832915488630...|\n",
       "| 0.068725586|  -3.0742188|       97.0|        1.6083984|       0.27520752|       42.937286|          0.18374634|           1.1877441|          24.936188|2.021832915488630...|     HH|                1| HH|1.0|2.021832915488630...|2.021832915488630...|0.006520355193267766|      2.021832915488630...|\n",
       "|    0.756958| -0.05404663|     98.875|       0.48364258|         2.505371|        41.58864|          -0.3071289|            3.104004|          40.117764|2.021832915488630...|     HH|                2| HH|1.0|2.021832915488630...|2.021832915488630...|0.006520355193267766|      2.021832915488630...|\n",
       "|   2.1328125|   -2.772461|     148.75|        1.0141602|         0.833374|       77.491875|           1.3835449|           1.4135742|          29.633177|2.021832915488630...|     HH|                3| HH|1.0|2.021832915488630...|2.021832915488630...|0.006520355193267766|      2.021832915488630...|\n",
       "| -0.57177734| -0.19345093|     222.25|       -1.0063477|       -2.1157227|        87.77944|         -0.58496094|           -2.224121|          21.291397|2.021832915488630...|     HH|                4| HH|1.0|2.021832915488630...|2.021832915488630...|0.006520355193267766|      2.021832915488630...|\n",
       "+------------+------------+-----------+-----------------+-----------------+----------------+--------------------+--------------------+-------------------+--------------------+-------+-----------------+---+---+--------------------+--------------------+--------------------+--------------------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create custom function\n",
    "def weightSum(ttag):\n",
    "    return df.select(F.sum(df.filter(df.tag == ttag).event_weight)).collect()[0][0]\n",
    "\n",
    "# and normalize the training weight\n",
    "for ttag in tags:\n",
    "    # training weight *= Nevents / sum of event weight\n",
    "    df = df.withColumn(\"training_weight_\"+ttag, F.when(df.tag == ttag, df.training_weight * df.count() / weightSum(ttag)).otherwise(df.training_weight))\n",
    "# concatenate\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572075ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d130c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['weight']>0) & (df['training_weight'] < 200)]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "importlib.reload(utils) # Reload in case file has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d873e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Using event weight')\n",
    "utils.checkBatches(df, label_column='tag', weight_column='event_weight', batch_size=parameters['batch_size'])\n",
    "print ('Using training weight')\n",
    "utils.checkBatches(df, label_column='tag', weight_column='training_weight', batch_size=parameters['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e26d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the background and signal weights #\n",
    "fig,axs = plt.subplots(figsize=(16,8),nrows=len(tags),ncols=3)\n",
    "fig.subplots_adjust(left=0.1, right=0.9, top=0.98, bottom=0.1, wspace=0.2,hspace=0.4)\n",
    "for irow,tag in enumerate(tags):\n",
    "    for icol,column in enumerate(['weight','event_weight','training_weight']):\n",
    "        axs[irow,icol].hist(df[df['tag']==tag][column],bins=100,color='b')\n",
    "        axs[irow,icol].set_title(f\"Category = {tag}\")\n",
    "        axs[irow,icol].set_xlabel(column)\n",
    "        axs[irow,icol].set_yscale('log')\n",
    "fig.savefig(\"event_weights_A.pdf\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aafcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine splitting variable #\n",
    "split_var = np.abs(df['leadingLepton_phi'].copy())\n",
    "split_var *= 1e4\n",
    "split_var -= np.floor(split_var)\n",
    "split_var = (split_var*1e1).astype(int)\n",
    "split_var = split_var %2 == 0\n",
    "print (f'Even set has {df[split_var].shape[0]:10d} events [{df[split_var].shape[0]/df.shape[0]*100:5.2f}%]')\n",
    "print (f'Odd  set has {df[~split_var].shape[0]:10d} events [{df[~split_var].shape[0]/df.shape[0]*100:5.2f}%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f60222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets splitting #\n",
    "print (f'Using split type {split}')\n",
    "if split == 'even':\n",
    "    train_df = df[~split_var] # Trained on odd\n",
    "    test_df  = df[split_var]  # Evaluated on even \n",
    "elif split == 'odd':\n",
    "    train_df = df[split_var]  # Trained on even\n",
    "    test_df  = df[~split_var] # Evaluated on odd \n",
    "else:\n",
    "    raise RuntimeError(f'Split needs to be either odd or even, is {split}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ced49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize for training\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile corrections #\n",
    "# When an event has a large weight, it can imbalance a lot the training, still the weight might have a meaning\n",
    "# Idea : instead of 1 event with wi>>1, we use N copies of the event with wf = wi/N\n",
    "# From the point of view of the physics it does not matter, the total event weight sum of each process is the same\n",
    "# From the point of view of the DNN, we have split a tough nut to crack into several smaller ones\n",
    "\n",
    "quantile_lim = train_df['training_weight'].quantile(1)\n",
    "print (f'{(1-quantile)*100:5.2f}% right quantile is when weight is at {quantile_lim}')\n",
    "print ('  -> These events will be repeated and their learning weights reduced accordingly to avoid unstability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the events #\n",
    "idx_to_repeat = train_df['training_weight'] >= quantile_lim\n",
    "events_excess = train_df[idx_to_repeat].copy()\n",
    "saved_columns = train_df[['training_weight','process']].copy()\n",
    "# Compute multiplicative factor #\n",
    "factor = (events_excess['training_weight']/quantile_lim).values.astype(np.int32)\n",
    "# Correct the weights of events already in df #\n",
    "train_df.loc[idx_to_repeat,'training_weight'] /= factor\n",
    "# Add N-1 copies #\n",
    "arr_to_repeat = train_df[idx_to_repeat].values\n",
    "repetition = np.repeat(np.arange(arr_to_repeat.shape[0]), factor-1)\n",
    "df_repeated = pd.DataFrame(np.take(arr_to_repeat,repetition,axis=0),columns=train_df.columns)\n",
    "df_repeated = df_repeated.astype(train_df.dtypes.to_dict()) # otherwise dtypes are object\n",
    "train_df = pd.concat((train_df,df_repeated),axis=0,ignore_index=True).sample(frac=1).reset_index() # Add and randomize\n",
    "# Printout #\n",
    "print ('Changes per process in training set')\n",
    "for process in pd.unique(train_df['process']):\n",
    "    N_before = saved_columns[saved_columns['process']==process].shape[0]\n",
    "    N_after  = train_df[train_df['process']==process].shape[0]\n",
    "    if N_before != N_after:\n",
    "        print (f\"{process:20s}\")\n",
    "        print (f\"... {N_before:6d} events [sum weight = {saved_columns[saved_columns['process']==process]['training_weight'].sum():14.6f}]\",end=' -> ')\n",
    "        print (f\"{N_after:6d} events [sum weight = {train_df[train_df['process']==process]['training_weight'].sum():14.6f}]\")\n",
    "print (f\"Total entries : {saved_columns.shape[0]:14d} -> {train_df.shape[0]:14d}\")\n",
    "print (f\"Total event sum : {saved_columns['training_weight'].sum():14.6f} -> {train_df['training_weight'].sum():14.6f}\")\n",
    "\n",
    "# Validation split #\n",
    "train_df,val_df  = train_test_split(train_df,test_size=0.3)\n",
    "\n",
    "# Printout #\n",
    "print ('\\nFinal sets')\n",
    "print (f'Training set   = {train_df.shape[0]}')\n",
    "print (f'Validation set = {val_df.shape[0]}')\n",
    "print (f'Testing set    = {test_df.shape[0]}')\n",
    "print (f'Total set      = {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76336e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the background and signal weights #\n",
    "fig,axs = plt.subplots(figsize=(16,8),nrows=1,ncols=2)\n",
    "fig.subplots_adjust(left=0.1, right=0.9, top=0.96, bottom=0.1, wspace=0.2,hspace=0.3)\n",
    "\n",
    "if split == 'even':\n",
    "    axs[0].hist(df[~split_var]['training_weight'],bins=100,color='b')\n",
    "elif split == 'odd':\n",
    "    axs[0].hist(df[split_var]['training_weight'],bins=100,color='b')\n",
    "axs[0].set_title(\"Before correction\")\n",
    "axs[0].set_xlabel(\"Training weight\")\n",
    "axs[0].set_yscale('log')\n",
    "axs[1].hist(train_df['training_weight'],bins=100,color='b')\n",
    "axs[1].set_title(\"After correction\")\n",
    "axs[1].set_xlabel(\"Training weight\")\n",
    "axs[1].set_yscale('log')\n",
    "fig.savefig(\"event_weights_C.pdf\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd6ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer #\n",
    "inputs = keras.Input(shape=(len(input_vars),), name=\"particles\")\n",
    "# Preprocessing layer\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "normalizer = preprocessing.Normalization(mean     = train_df[input_vars].mean(axis=0),\n",
    "                                        variance = train_df[input_vars].var(axis=0),\n",
    "                                        name     = 'Normalization')(inputs)\n",
    "    # this layer does the preprocessing (x-mu)/std for each input\n",
    "# Dense (hidden) layers #\n",
    "x = normalizer\n",
    "for i in range(parameters['n_layers']):\n",
    "    x = layers.Dense(units                = parameters['n_neurons'], \n",
    "                    activation           = parameters['hidden_activation'], \n",
    "                    activity_regularizer = tf.keras.regularizers.l2(parameters['l2']),\n",
    "                    name                 = f\"dense_{i}\")(x)\n",
    "    if parameters['batch_norm']:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    if parameters['dropout'] > 0.:\n",
    "        x = layers.Dropout(parameters['dropout'])(x)\n",
    "# Output layer #\n",
    "outputs = layers.Dense(units                = 2, \n",
    "                    activation           = parameters['output_activation'],\n",
    "                    activity_regularizer = tf.keras.regularizers.l2(parameters['l2']),\n",
    "                    name                 = \"predictions\")(x)\n",
    "\n",
    "# Registering the model #\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preprocess = keras.Model(inputs=inputs, outputs=normalizer)\n",
    "out_test = model_preprocess.predict(train_df[input_vars],batch_size=5000)\n",
    "print ('Input (after normalization) mean (should be close to 0)')\n",
    "print (out_test.mean(axis=0))\n",
    "print ('Input (after normalization) variance (should be close to 1)')\n",
    "print (out_test.var(axis=0))\n",
    "\n",
    "model.compile(\n",
    "    #optimizer=keras.optimizers.RMSprop(),\n",
    "    optimizer=keras.optimizers.legacy.Adam(learning_rate=parameters['lr']),  # Optimizer\n",
    "    # Loss function to minimize\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    # List of metrics to monitor\n",
    "    metrics=[keras.metrics.BinaryAccuracy(),\n",
    "            tf.keras.metrics.AUC(),\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall()],\n",
    "    weighted_metrics=[]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5028e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Callbacks #\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss',\n",
    "                            min_delta = 0.001, \n",
    "                            patience = 20,\n",
    "                            verbose=1,\n",
    "                            mode='min',\n",
    "                            restore_best_weights=True)\n",
    "# Stop the learning when val_loss stops increasing \n",
    "# https://keras.io/api/callbacks/early_stopping/\n",
    "\n",
    "reduce_plateau = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                                factor = 0.1,\n",
    "                                min_delta = 0.001, \n",
    "                                patience = 8,\n",
    "                                min_lr = 1e-8,\n",
    "                                verbose=2,\n",
    "                                mode='min')\n",
    "# reduce LR if not improvement for some time \n",
    "# https://keras.io/api/callbacks/reduce_lr_on_plateau/\n",
    "import History \n",
    "importlib.reload(History)\n",
    "loss_history = History.LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10528faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_df[input_vars],\n",
    "    train_df[tags],\n",
    "    verbose=2,\n",
    "    batch_size=parameters['batch_size'],\n",
    "    epochs=parameters['epochs'],\n",
    "    sample_weight=train_df['training_weight'],\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(val_df[input_vars],val_df[tags],val_df['training_weight']),\n",
    "    callbacks = [early_stopping, reduce_plateau, loss_history],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f5ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "History.PlotHistory(loss_history,params=parameters,outputName=f'loss_{suffix}_{split}.png') # giving an error i.e.\n",
    "# Params is a dict of parameters with name and values\n",
    "# used for plotting\n",
    "print(\"printed history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaee362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce output on the test set as new column #\n",
    "output = model.predict(test_df[input_vars],batch_size=5000)\n",
    "\n",
    "output_tags = [f'output {tag}' for tag in tags]\n",
    "    # Here the batch_size arg is independent of the learning\n",
    "    # Default is 32, but it can become slow, by using large value it will just compute more values in parallel\n",
    "    # (more or less parallel, we are not using a GPU)\n",
    "for output_tag in output_tags:\n",
    "    if output_tag in test_df.columns:\n",
    "        # If already output, need to remove to add again\n",
    "        # avoid issues in case you run this cell multiple times\n",
    "        del test_df[output_tag]\n",
    "\n",
    "test_df = pd.concat((test_df,pd.DataFrame(output,columns=output_tags,index=test_df.index)),axis=1)\n",
    "# We add the output as a column, a bit messy, different ways, here I use a concatenation\n",
    "\n",
    "# Make the discriminator #\n",
    "if 'd_HH' in test_df.columns:\n",
    "    del test_df['d_HH']\n",
    "    \n",
    "signal_idx = [i for i,tag in enumerate(tags) if 'HH' in tag]\n",
    "\n",
    "# d_HH = ln (P(HH) / (P(single H) + P(background)))\n",
    "\n",
    "#test_df['d_HH'] = pd.Series(np.ones(test_df.shape[0]))\n",
    "\n",
    "# Numerator #\n",
    "num = pd.DataFrame((test_df[[output_tags[i] for i in range(len(tags)) if i in signal_idx]]).sum(axis=1))\n",
    "# Denominator #\n",
    "den = pd.DataFrame(test_df[[output_tags[i] for i in range(len(tags)) if i not in signal_idx]].sum(axis=1))\n",
    "# Ln #\n",
    "d_HH = np.log(num / den)\n",
    "test_df['d_HH'] = d_HH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb051e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ROC curves\")\n",
    "import roc\n",
    "importlib.reload(roc) # Reload in case file has changed\n",
    "for tag in tags:\n",
    "    print (f'ROC curve of binary classification of {tag} node versus all the others')\n",
    "    roc.rocAndSig(y_true                 = test_df[tag],\n",
    "                y_pred                 = test_df[f'output {tag}'],\n",
    "                w_roc                  = test_df['training_weight'],\n",
    "                w_sig                  = test_df['event_weight'],\n",
    "                show_significance      = 'HH' in tag,\n",
    "                outputName             = f'roc_{suffix}_{split}_{tag}.pdf')\n",
    "\n",
    "# Multiclassification ROC curves are a bit harder to interpret than binary classification\n",
    "# Here I do one versus the rest, so each ROC curves shows how the DNN is able to classify\n",
    "# one class (HH, single H or background) versus all the others, which is one projection on\n",
    "# how to see the performances\n",
    "# For HH I show the significance but more as an information, because using only the HH node \n",
    "# means we do not use all the power of the multiclass (-> d_HH is for that)\n",
    "print (f'ROC curve of binary classification of d_HH')\n",
    "roc.rocAndSig(y_true                 = test_df['HH'],\n",
    "            y_pred                 = test_df['d_HH'],\n",
    "            w_roc                  = test_df['training_weight'],\n",
    "            w_sig                  = test_df['event_weight'],\n",
    "            show_significance      = True,\n",
    "            outputName             = f'roc_{suffix}_{split}_d_HH.pdf')\n",
    "\n",
    "# Tryign a new things, seeing the discrimination power of each node, class wise\n",
    "for tag in tags:\n",
    "    print (f'Multi roc curve for `output {tag}`')\n",
    "    tags_order = [tag] + [t for t in tags if t != tag]\n",
    "    roc.multiRoc(outputs    = [test_df[test_df['tag']==tag][f'output {tag}'] for tag in tags_order],\n",
    "                tags       = tags_order,\n",
    "                weights    = [test_df[test_df['tag']==tag]['training_weight'] for tag in tags_order],\n",
    "                title      = f'Using node {tag}',\n",
    "                outputName = f'multi_roc_{suffix}_{split}_output_{tag}.pdf')\n",
    "    \n",
    "fig,axs = plt.subplots(figsize=(12,25),nrows=len(tags)+1,ncols=2)\n",
    "fig.subplots_adjust(left=0.1, right=0.9, top=0.98, bottom=0.1, wspace=0.3,hspace=0.5)\n",
    "\n",
    "tag_df = {tag:test_df[test_df['tag']==tag] for tag in tags}\n",
    "colors = ['g','r','b']\n",
    "\n",
    "# Manual binning so we can compute significance #\n",
    "n_bins = 50\n",
    "\n",
    "def get_bin_content(bins,y,w):\n",
    "    digitized = np.digitize(y,bins)\n",
    "    return np.array([w[digitized==i].sum() for i in range(1, len(bins))])\n",
    "\n",
    "for irow,output_tag in enumerate(output_tags+['d_HH']):\n",
    "    for icol,weight in enumerate(['event_weight','training_weight']):\n",
    "        # Fill the bins myself #\n",
    "        bins = np.linspace(test_df[output_tag].min(),test_df[output_tag].max(),n_bins+1)\n",
    "        centers = (bins[1:]+bins[:-1])/2\n",
    "        widths = np.diff(bins)\n",
    "        \n",
    "        tag_content = {tag:get_bin_content(bins,tag_df[tag][output_tag],tag_df[tag][weight])for tag in tags}\n",
    "        tag_cumsum_left = {tag:np.cumsum(tag_content[tag])/tag_content[tag].sum() for tag in tags}\n",
    "        tag_cumsum_right = {tag:np.cumsum(tag_content[tag][::-1])[::-1]/tag_content[tag].sum() for tag in tags}\n",
    "        # Need to integrate all the bins right of the DNN cut to get significance\n",
    "        #z_left = np.nan_to_num(np.sqrt(2*((cumsum_s_left+cumsum_b_left)*np.log(1+cumsum_s_left/cumsum_b_left)-cumsum_s_left)))\n",
    "        #z_right = np.nan_to_num(np.sqrt(2*((cumsum_s_right+cumsum_b_right)*np.log(1+cumsum_s_right/cumsum_b_right)-cumsum_s_right)))\n",
    "        #z_left /= z_left.max()\n",
    "        #z_right /= z_right.max()\n",
    "        for i,(tag,content) in enumerate(tag_content.items()):\n",
    "            axs[irow,icol].bar(x=centers,height=content,width=widths,fill=False,edgecolor=colors[i],label=tag)     \n",
    "        #ax2=axs[irow,icol].twinx()   \n",
    "        \n",
    "        #ax2.plot(centers,z_left,color='r',label='Significance (left of cut) [normed]')\n",
    "        #ax2.plot(centers,z_right,color='r',linestyle='--',label='Significance (right of cut) [normed]')\n",
    "\n",
    "        #for i,tag in enumerate(tag_content.keys()):\n",
    "        #    ax2.plot(centers,content,color=colors[i],linestyle='-',label=f'{tag} content (left of cut)')\n",
    "        #    ax2.plot(centers,color=colors[i],linestyle='--',label=f'{tag} content (right of cut)')\n",
    "        \n",
    "        #ax2.set_yscale(\"log\")\n",
    "        #ax2.set_ylim([0,1.4])\n",
    "        #ax2.set_ylabel('Cumulative distribution')\n",
    "        #ax2.legend(loc='upper right')\n",
    "\n",
    "        axs[irow,icol].set_title(f\"Using {weight}\")\n",
    "        axs[irow,icol].set_xlabel(output_tag)\n",
    "        axs[irow,icol].set_ylabel('Yield')\n",
    "        axs[irow,icol].set_ylim(1e-5,max([content.max() for content in tag_content.values()])*100)\n",
    "        axs[irow,icol].set_yscale('log')\n",
    "        axs[irow,icol].legend(loc='upper left')\n",
    "fig.savefig(f\"prediction_{suffix}_{split}.pdf\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea08ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "print(\"evaluate the model...\")\n",
    "scores = model.evaluate(test_df[input_vars], \n",
    "                        test_df[tags], \n",
    "                        sample_weight = test_df['training_weight'], \n",
    "                        batch_size = 5000,\n",
    "                        verbose=2)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "modelName = f\"model_{suffix}_{split}\"\n",
    "model.save(modelName)\n",
    "print(f\"Saved model to disk as {modelName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85a146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c050f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
